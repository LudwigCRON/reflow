#!/usr/bin/env python3
# coding: utf-8

import os
import re
import sys
import time
import argparse

# assume the run utility is placed in the envs/bin/ directory
ENVBIN_DIR = os.path.dirname(os.path.realpath(__file__))
# define the path where is stored the flow
REFLOW_DIR = os.environ["REFLOW"]
sys.path.append(REFLOW_DIR)

import common.relog
import common.config
import common.utils as utils


def living_log_stream(filepath: str, stop_keyword: str):
    if not os.path.exists(filepath):
        return None
    with open(filepath, "rt") as fp:
        should_stop = False
        while not should_stop:
            line = fp.readline()
            if not line:
                time.sleep(1)
                continue
            should_stop = stop_keyword in line
            yield line


def static_log_stream(filepath: str):
    if not os.path.exists(filepath):
        return None
    with open(filepath, "rt") as fp:
        for line in fp:
            yield line


def filter_stream(
    stream, rules: dict = {}, current_task: utils.doit.RelogTask = None, db_path: str = ""
):
    needed_buffer = max([rule.get("buffer", 1) for rule in rules.values()])
    counts = {rule: 0 for rule in rules}
    should_stop = False
    buffer = []
    for line in stream:
        buffer.append(line)
        replaced = False
        if len(buffer) == needed_buffer:
            for type, rule in rules.items():
                matches = re.finditer(
                    rule.get("regex", ""), "".join(buffer), re.MULTILINE | re.DOTALL
                )
                for match in matches:
                    # get results and add rule name as type
                    res = match.groupdict()
                    res["type"] = type.upper()
                    # clean up spurious '\n' or spaces
                    for k, v in res.items():
                        res[k] = v.strip()
                    s = rule.get("format", "").format(**res)
                    # erase previous printed line use in the buffer
                    print("\033[F\033[K" * (rule.get("buffer", 1) - 1), end="")
                    print(s)
                    replaced = True
                    # increment counters and register in db
                    counts[type] += 1
                    if db_path and "warn" in type.lower():
                        current_task.warning(s, db_path)
                    elif db_path and "err" in type.lower():
                        current_task.error(s, db_path)
                    elif db_path and "fat" in type.lower():
                        current_task.fatal(s, db_path)
                    # check limit
                    limit = rule.get("limit", 10000)
                    if counts[type] > limit:
                        msg = f"Exceeded maximum number of {type.upper()} (max: {limit})"
                        print(msg)
                        if db_path:
                            current_task.fatal(msg, db_path)
                        should_stop = True
                        break
            if should_stop:
                break
            buffer = buffer[1:]
        if not replaced:
            print(line, end="")
    # print a small summary
    print("==== Summary ====")
    print("simulation done with")
    max_len_type = max([len(type) for type in counts])
    for type in counts:
        nb_spaces = max_len_type - len(type) + 1
        print(f"{type.upper()}:{' '*nb_spaces}{counts[type]}")


def main():
    # define db path
    default_db_path = common.relog.get_relog_dbpath()
    # get tools name from cli arguments
    parser = argparse.ArgumentParser(description="Reformatting Log Tool (ReLog)")
    parser.add_argument("toolname", help="name of the tool", default="")
    parser.add_argument(
        "task_name", help="task identifier in database", default="", nargs="?"
    )
    parser.add_argument(
        "--db", help="path to the db to store info", default=default_db_path
    )
    parser.add_argument(
        "-l", "--log", help="path to the log file to be analyzed", default=None
    )
    parser.add_argument("-s", "--stop", help="keyword for stopping parsing", default=None)
    cli_args = parser.parse_args()
    # if ask for cleanning
    if cli_args.toolname == "clean":
        if os.path.exists(cli_args.db):
            os.remove(cli_args.db)
        exit(0)
    # find project yaml to load tools paths
    common.config.load_config(
        utils.normpath(os.path.join(REFLOW_DIR, "tools")), os.getcwd()
    )
    # load the selected tool's config
    for tools_path in common.config.vault.get("tools_paths", []):
        common.config.load_tool_config(
            cli_args.toolname, f"{tools_path}/{cli_args.toolname}"
        )
    # get relog regex and format instructions
    tool_config = common.config.vault.get(cli_args.toolname)
    if "relog" not in tool_config:
        print("No filtering instructions")

    rules = tool_config.get("relog")
    # create default task object to save in db
    task_name, test_path = cli_args.task_name.split(":")[-2:]
    current_task = utils.doit.RelogTask(test_path, task_name, 0.0)
    # detect and filter stdin/living log file/static log file
    stream = sys.stdin
    if cli_args.log is not None and cli_args.stop is not None:
        stream = living_log_stream(cli_args.log, cli_args.stop)
    elif cli_args.log is not None:
        stream = static_log_stream(cli_args.log)
    filter_stream(stream, rules, current_task, cli_args.db)


if __name__ == "__main__":
    main()
